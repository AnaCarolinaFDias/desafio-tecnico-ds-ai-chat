{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from uuid import uuid4\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configurar o logging para salvar a saída de debug em um arquivo\n",
    "logging.basicConfig(\n",
    "    filename='./logs/debug_output.log',  # O arquivo onde os logs serão salvos\n",
    "    level=logging.DEBUG,          # O nível de log (DEBUG para capturar tudo)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Formato do log\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # read local .env file\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'inputs/OutdoorClothingCatalog_1000_withCategories.csv'\n",
    "loader = CSVLoader(file_path=file, encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "\n",
    "split_documents = []\n",
    "for doc in docs:\n",
    "    split_docs = text_splitter.split_documents([doc])\n",
    "    split_documents.extend(split_docs)\n",
    "    \n",
    "documents_dict = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in split_documents\n",
    "]\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open(\"inputs/documents_split_langchain.json\", \"w\") as file:\n",
    "    json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "logger.debug(\"Documents have been saved to 'documents_split_langchain.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents split\n",
    "with open(\"inputs/documents_split_langchain.json\", \"r\") as file:\n",
    "    documents_dict = json.load(file)\n",
    "\n",
    "# Convert the list of dictionaries back to a list of Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n",
    "    for doc in documents_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import openai_embedding_function, hf_embeddings_function, google_embedding_function \n",
    "# Generating embeddings \n",
    "openai_embedding_function(documents, model = \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI db loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_provider = 'OpenAI'\n",
    "vector_store = load_VectorStore(embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as querys para validar os metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5):\n",
    "    \n",
    "    logger.debug(\"Creating {select_n_documents} validation examples with {llm_model} {embeddings_provider}\")\n",
    "\n",
    "    # Initialize the example generation chain\n",
    "    example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n",
    "\n",
    "    # Select 5 random documents from the list `docs`\n",
    "    random_documents = random.sample(docs, select_n_documents)\n",
    "\n",
    "    # Format documents for input\n",
    "    formatted_documents = [{\"doc\": doc} for doc in random_documents]\n",
    "\n",
    "    # Apply the example generation chain and parse the results\n",
    "    gen_examples = example_gen_chain.apply_and_parse(formatted_documents)\n",
    "\n",
    "    # Adjust the output to include context\n",
    "    gen_examples_adjusted_examples = []\n",
    "    for doc, example in zip(random_documents, gen_examples):\n",
    "        qa_pair = example.get('qa_pairs', {})\n",
    "        query = qa_pair.get('query', '')\n",
    "        answer = qa_pair.get('answer', '')\n",
    "\n",
    "        gen_examples_adjusted_examples.append({\n",
    "            'context': doc,  # Add the original document as context\n",
    "            'query': query,\n",
    "            'ground_truths': answer\n",
    "        })\n",
    "\n",
    "    validation_examples = gen_examples_adjusted_examples\n",
    "\n",
    "    # Prepare the results in the expected format\n",
    "    documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"ground_truths\": doc['ground_truths'], \n",
    "          \"query\": doc['query']}   \n",
    "         for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "    logger.debug(\"Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return validation_examples\n",
    "\n",
    "validation_examples = generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando métodos de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Documents have been saved to 'results/query_OpenAI_baselinemethod_documents_dict_results.json'\n"
     ]
    }
   ],
   "source": [
    "def baseline_similarity_method(validation_examples, vector_store, embeddings_provider):\n",
    "\n",
    "    logger.debug(\"baseline_similarity_method to answer queries iniciated\")\n",
    "\n",
    "    model = 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "    # Initialize the language model\n",
    "    llm = OpenAI(temperature=0.2, model=model)\n",
    "\n",
    "    # Set up the retriever with similarity search\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)  # Retrieve the most relevant chunk\n",
    "\n",
    "    # Initialize the RetrievalQA chain\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever, \n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\"document_separator\": \"\\n\"}\n",
    "    )\n",
    "\n",
    "    # Apply the model to the validation examples\n",
    "    predictions = qa.apply(validation_examples)\n",
    "\n",
    "    # Prepare the results\n",
    "    baselinemethod_documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"query\": doc['query'],\n",
    "          \"ground_truths\": doc['ground_truths'],  \n",
    "          \"answer\": doc['result']}\n",
    "         for doc in data]\n",
    "        for data in [predictions]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_baselinemethod_documents_dict_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(baselinemethod_documents_dict, file, indent=4)\n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(\"baseline_similarity_method Documents have been saved to {results_filename}\")\n",
    "\n",
    "\n",
    "    return baselinemethod_documents_dict\n",
    "\n",
    "baselinemethod_documents_dict = baseline_similarity_method(validation_examples, vector_store, embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica por similaridade incluindo na query categorias referentes a pergunta para a busca textual \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The Timeless Textured Stripe Cashmere Cardigan with Pocket would fall under the following categories:\n",
      "- Sweaters\n",
      "- Cardigans\n",
      "- Cashmere Clothing\n",
      "- Textured Clothing\n",
      "- Striped Clothing\n",
      "- Clothing with Pockets\n",
      "\n",
      "1. Outdoor furniture\n",
      "2. Garden benches\n",
      "3. Patio furniture\n",
      "4. Wood furniture\n",
      "5. Weather-resistant furniture\n",
      "6. Durable furniture\n",
      "7. Rust-resistant furniture\n",
      "8. Sturdy furniture\n",
      "9. Outdoor seating\n",
      "10. Wooden benches\n",
      "11. Garden decor\n",
      "12. Backyard furniture\n",
      "13. Park benches\n",
      "14. Commercial outdoor furniture\n",
      "15. Natural wood furniture\n",
      "\n",
      "1. Fishing gear\n",
      "2. Fly rods\n",
      "3. Outdoor equipment\n",
      "4. Sportswear\n",
      "5. Fishing rods\n",
      "6. Fly fishing accessories\n",
      "7. Fly fishing outfits\n",
      "8. Fishing outfits\n",
      "9. Fishing equipment\n",
      "10. Fishing accessories\n",
      "\n",
      "1. Hiking boots\n",
      "2. Women's footwear\n",
      "3. Outdoor gear\n",
      "4. Adventure gear\n",
      "5. All-terrain shoes\n",
      "6. Hiking equipment\n",
      "7. Keen Targhee II\n",
      "8. Women's shoes\n",
      "9. Outdoor footwear\n",
      "10. Hiking essentials\n",
      "\n",
      "1. Bedding \n",
      "2. Quilts \n",
      "3. Thermal Quilts \n",
      "4. Bedding Dimensions \n",
      "5. Bed Sizes \n",
      "6. Comfort-enhancing Features \n",
      "7. Outdoor Clothing \n",
      "8. Camping Gear \n",
      "9. Cold Weather Gear \n",
      "10. Sleeping Gear\n",
      "Documents have been saved to 'results/query_OpenAI_contextualize_query_with_categories_dict_results.json'\n"
     ]
    }
   ],
   "source": [
    "def contextualize_query_with_categories(vector_store, validation_examples,embeddings_provider):\n",
    "\n",
    "    logger.debug(\"contextualized_query to answer queries iniciated\")\n",
    "\n",
    "    # Define the high-level query template for category search\n",
    "    category_prompt = \"\"\"\n",
    "    You are an expert in outdoor clothing products. Based on the following query, \n",
    "    return a list of relevant categories (e.g., jackets, pants) from the catalog.\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    category_template = PromptTemplate(input_variables=[\"query\"], template=category_prompt)\n",
    "    category_chain = LLMChain(prompt=category_template, llm=OpenAI())\n",
    "\n",
    "    # Define the refined search for documents within each category\n",
    "    document_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "    Your function is to use the catalog of retrieved context to answer the client's questions based on the following query and the category {category}.\n",
    "    Query: {query}\n",
    "    Category: {category}\n",
    "    Catalog: {catalog}\n",
    "    \"\"\"\n",
    "\n",
    "    document_template = PromptTemplate(input_variables=[\"query\", \"category\",'catalog'], template=document_prompt)\n",
    "    document_chain = LLMChain(prompt=document_template, llm=OpenAI())\n",
    "\n",
    "    for index, item in enumerate(validation_examples):\n",
    "        query = item['query']\n",
    "        category_result = category_chain.run({\"query\": query})\n",
    "\n",
    "        modified_query = f\"{query} Category: {category_result}\"  # Combine query and category for search\n",
    "        catalog = vector_store.similarity_search(modified_query, k=5) \n",
    "\n",
    "        print(category_result)\n",
    "        refined_answer = document_chain.run({\"query\": query, \"category\": category_result, \"catalog\": catalog})\n",
    "        validation_examples[index]['result'] = refined_answer\n",
    "\n",
    "    contextualize_query_with_categories_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "        \"metadata\": doc['context'].metadata,\n",
    "        \"query\": doc['query'],\n",
    "        \"ground_truths\": doc['ground_truths'],  \n",
    "        \"answer\": doc['result'],\n",
    "        } \n",
    "        for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the list of dictionaries to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_contextualize_query_with_categories_dict_results.json\"\n",
    "\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(contextualize_query_with_categories_dict, file, indent=4)\n",
    "        \n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(\"Contextualized query method Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return contextualize_query_with_categories_dict\n",
    "\n",
    "# Assuming vector_store is defined as per the previous setup\n",
    "contextualize_query_with_categories_dict = contextualize_query_with_categories(vector_store, validation_examples,embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Retrieval Strategy (Hierarchical Retrieval)\n",
    "Hierarchical retrieval means you will first retrieve high-level categories or subtopics and then refine the search to get more specific documents.\n",
    "\n",
    "Step 1: Retrieve relevant high-level categories (e.g., product types like \"jackets\", \"pants\", etc.)\n",
    "Step 2: Within each category, retrieve specific documents (e.g., details of specific jackets or pants).\n",
    "To achieve this, you may use a two-step retrieval process. First, search for broad categories in the vector store, then use the results to refine the search.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica por similaridade incluindo filtro de documentos por categorias similares para a busca textual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to 'results/query_OpenAI_hierarchicalmethod_documents_dict_results.json'\n"
     ]
    }
   ],
   "source": [
    "def hierarchical_retrieval_with_categories(vector_store, validation_examples,embeddings_provider):\n",
    "\n",
    "    logger.debug(\"Hierarchical retrieval method to answer queries initiated\")\n",
    "\n",
    "    # Define the high-level query template for category search\n",
    "    category_prompt = \"\"\"\n",
    "    You are an expert in outdoor clothing products. Based on the following query, \n",
    "    return a list of relevant categories (e.g., jackets, pants) from the catalog.\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    category_template = PromptTemplate(input_variables=[\"query\"], template=category_prompt)\n",
    "    category_chain = LLMChain(prompt=category_template, llm=OpenAI())\n",
    "\n",
    "    # Define the refined search for documents within each category\n",
    "    document_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "    Your function is to use the catalog of retrieved context to answer the client's questions based on the following query.\n",
    "    Query: {query}\n",
    "    Catalog: {catalog}\n",
    "    \"\"\"\n",
    "\n",
    "    document_template = PromptTemplate(input_variables=[\"query\", 'catalog'], template=document_prompt)\n",
    "    document_chain = LLMChain(prompt=document_template, llm=OpenAI())\n",
    "\n",
    "    for index, item in enumerate(validation_examples):\n",
    "        query = item['query']\n",
    "        category_result = category_chain.run({\"query\": query})\n",
    "    \n",
    "        # Convert category_result to vector (embedding) using the same model as the vector store\n",
    "        relevant_documents = vector_store.similarity_search(category_result, k=5)\n",
    "\n",
    "        # If no relevant documents are found, you can skip this iteration or handle accordingly\n",
    "        if not relevant_documents:\n",
    "            print(f\"No relevant documents found for category '{category_result}'\")\n",
    "            continue\n",
    "        \n",
    "        # Proceed with refining the answer using the filtered documents\n",
    "        refined_answer = document_chain.run({\"query\": query, \"catalog\": relevant_documents})\n",
    "        validation_examples[index]['result'] = refined_answer\n",
    "\n",
    "    hierarchicalmethod_documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"query\": doc['query'],\n",
    "          \"ground_truths\": doc['ground_truths'],  \n",
    "          \"answer\": doc['result'],\n",
    "        } \n",
    "        for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the list of dictionaries to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_hierarchicalmethod_documents_dict_results.json\"\n",
    "\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(hierarchicalmethod_documents_dict, file, indent=4)\n",
    "        \n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(f\"Hierarchical method Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return hierarchicalmethod_documents_dict\n",
    "\n",
    "\n",
    "# Assuming vector_store is defined as per the previous setup\n",
    "hierarchicalmethod_documents_dict = hierarchical_retrieval_with_categories(vector_store, validation_examples,embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica de Recuperação-Para-Geração (RAG) com dois estágios\n",
    "\n",
    "###  classificação de relevância de documentos e resposta gerada baseada nos documentos relevantes usando o ChatGroq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 44\u001b[0m, in \u001b[0;36mgrade_and_retrieve_documents\u001b[1;34m(docs, question)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m future \u001b[38;5;129;01min\u001b[39;00m as_completed(futures):\n\u001b[1;32m---> 44\u001b[0m     doc, res \u001b[38;5;241m=\u001b[39m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m res\u001b[38;5;241m.\u001b[39mOrdinalScore \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m3\u001b[39m:  \u001b[38;5;66;03m# Keep documents with score greater than 3\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_condition\u001b[38;5;241m.\u001b[39mwait(timeout)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "Cell \u001b[1;32mIn[166], line 16\u001b[0m, in \u001b[0;36mgrade_document\u001b[1;34m(doc, question, llm, grade_prompt)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrade_document\u001b[39m(doc, question, llm, grade_prompt):\n\u001b[1;32m---> 16\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mgrade_prompt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquestion\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m doc, res\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:3024\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   3023\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3024\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[0;32m   3025\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5354\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m   5348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m   5349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5350\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[0;32m   5351\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   5352\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[0;32m   5353\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[1;32m-> 5354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5355\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5356\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5357\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:286\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    283\u001b[0m config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    285\u001b[0m     ChatGeneration,\n\u001b[1;32m--> 286\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    287\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    288\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    290\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    291\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    292\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    294\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    296\u001b[0m )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:786\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    785\u001b[0m prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 786\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:643\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    642\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[1;32m--> 643\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    644\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    645\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[0;32m    646\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[0;32m    647\u001b[0m ]\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:633\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 633\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[43m            \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    635\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    637\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    639\u001b[0m     )\n\u001b[0;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:851\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 851\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_groq\\chat_models.py:474\u001b[0m, in \u001b[0;36mChatGroq._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m    470\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    472\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    473\u001b[0m }\n\u001b[1;32m--> 474\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response)\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:298\u001b[0m, in \u001b[0;36mCompletions.create\u001b[1;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03mCreates a model response for the given chat conversation.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;124;03m  timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 298\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/openai/v1/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    302\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    303\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    304\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    312\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    313\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    314\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    315\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    328\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    330\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    332\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1263\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1260\u001b[0m opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1261\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1262\u001b[0m )\n\u001b[1;32m-> 1263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:955\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    953\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 955\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1043\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1042\u001b[0m     err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m-> 1043\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_retry_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[0;32m   1053\u001b[0m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1092\u001b[0m, in \u001b[0;36mSyncAPIClient._retry_request\u001b[1;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1090\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(timeout)\n\u001b[1;32m-> 1092\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\groq\\_base_client.py:1058\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1057\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1061\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1062\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[0;32m   1067\u001b[0m )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `llama-3.1-8b-instant` in organization `org_01je22c6rteh19msad031nzfr0` on requests per minute (RPM): Limit 30, Used 30, Requested 1. Please try again in 1.997s. Visit https://console.groq.com/docs/rate-limits for more information.', 'type': 'requests', 'code': 'rate_limit_exceeded'}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 92\u001b[0m\n\u001b[0;32m     89\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProduct with sun protection\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Run the main function\u001b[39;00m\n\u001b[1;32m---> 92\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[166], line 83\u001b[0m, in \u001b[0;36mmain\u001b[1;34m(docs, question)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m(docs, question):\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;66;03m# Step 1: Grade and retrieve relevant documents\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m     docs_to_use \u001b[38;5;241m=\u001b[39m \u001b[43mgrade_and_retrieve_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;66;03m# Step 2: Generate the final answer using the relevant documents\u001b[39;00m\n\u001b[0;32m     86\u001b[0m     generate_answer(docs_to_use, question)\n",
      "Cell \u001b[1;32mIn[166], line 41\u001b[0m, in \u001b[0;36mgrade_and_retrieve_documents\u001b[1;34m(docs, question)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Use ThreadPoolExecutor to parallelize document grading\u001b[39;00m\n\u001b[0;32m     40\u001b[0m docs_to_use \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 41\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mexecutor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrade_document\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretrieval_grader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:647\u001b[0m, in \u001b[0;36mExecutor.__exit__\u001b[1;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[1;32m--> 647\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    648\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\thread.py:235\u001b[0m, in \u001b[0;36mThreadPoolExecutor.shutdown\u001b[1;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads:\n\u001b[1;32m--> 235\u001b[0m         \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1112\u001b[0m, in \u001b[0;36mThread.join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1109\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot join current thread\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1114\u001b[0m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[0;32m   1115\u001b[0m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_for_tstate_lock(timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:1132\u001b[0m, in \u001b[0;36mThread._wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1133\u001b[0m         lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m   1134\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Data model\n",
    "# class GradeDocuments(BaseModel):\n",
    "#     \"\"\"Ordinal score for relevance check on retrieved documents.\"\"\"\n",
    "#     OrdinalScore: int = Field(\n",
    "#         description=\"Score between 1 (low) and 5 (high)\"\n",
    "#     )\n",
    "\n",
    "# # Function to grade a document\n",
    "# def grade_document(doc, question, llm, grade_prompt):\n",
    "#     res = grade_prompt.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#     return doc, res\n",
    "\n",
    "# # Function to grade and retrieve relevant documents in parallel\n",
    "# def grade_and_retrieve_documents(docs, question):\n",
    "#     # LLM with function call\n",
    "#     llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "#     structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "#     # Prompt for grading documents\n",
    "#     system = \"\"\"You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "#     Your function is to use the catalog of retrieved context to answer the client's questions based on the question.\n",
    "#     Give a OrdinalScore score '1 (low) and 5 (high)' to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "#     grade_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system),\n",
    "#             (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "#     # Use ThreadPoolExecutor to parallelize document grading\n",
    "#     docs_to_use = []\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(grade_document, doc, question, llm, retrieval_grader) for doc in docs]\n",
    "#         for future in as_completed(futures):\n",
    "#             doc, res = future.result()\n",
    "#             if res.OrdinalScore > 3:  # Keep documents with score greater than 3\n",
    "#                 docs_to_use.append(doc)\n",
    "\n",
    "#     return docs_to_use\n",
    "\n",
    "# # Function to format documents for the next step\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# # Function to generate the final response using the relevant documents\n",
    "# def generate_answer(docs_to_use, question):\n",
    "#     # Prompt for answering the question based on relevant documents\n",
    "#     system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "#     Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system),\n",
    "#             (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # LLM for answering the question\n",
    "#     llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "#     # Post-processing to parse the output\n",
    "#     rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#     # Format documents for input\n",
    "#     formatted_docs = format_docs(docs_to_use)\n",
    "\n",
    "#     # Generate the answer\n",
    "#     generation = rag_chain.invoke({\"documents\": formatted_docs, \"question\": question})\n",
    "#     print(generation)\n",
    "\n",
    "# # Main function to run the entire flow\n",
    "# def main(docs, question):\n",
    "#     # Step 1: Grade and retrieve relevant documents\n",
    "#     docs_to_use = grade_and_retrieve_documents(docs, question)\n",
    "    \n",
    "#     # Step 2: Generate the final answer using the relevant documents\n",
    "#     generate_answer(docs_to_use, question)\n",
    "\n",
    "# # Example usage\n",
    "# question = 'Product with sun protection'\n",
    "\n",
    "# # Run the main function\n",
    "# main(docs, question)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5):\n",
    "    \n",
    "    logger.debug(\"Creating {select_n_documents} validation examples with {llm_model} {embeddings_provider}\")\n",
    "\n",
    "    # Initialize the example generation chain\n",
    "    example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n",
    "\n",
    "    # Select 5 random documents from the list `docs`\n",
    "    random_documents = random.sample(docs, select_n_documents)\n",
    "\n",
    "    # Format documents for input\n",
    "    formatted_documents = [{\"doc\": doc} for doc in random_documents]\n",
    "\n",
    "    # Apply the example generation chain and parse the results\n",
    "    gen_examples = example_gen_chain.apply_and_parse(formatted_documents)\n",
    "\n",
    "    # Adjust the output to include context\n",
    "    gen_examples_adjusted_examples = []\n",
    "    for doc, example in zip(random_documents, gen_examples):\n",
    "        qa_pair = example.get('qa_pairs', {})\n",
    "        query = qa_pair.get('query', '')\n",
    "        answer = qa_pair.get('answer', '')\n",
    "\n",
    "        gen_examples_adjusted_examples.append({\n",
    "            'context': doc,  # Add the original document as context\n",
    "            'query': query,\n",
    "            'ground_truths': answer\n",
    "        })\n",
    "\n",
    "    validation_examples = gen_examples_adjusted_examples\n",
    "\n",
    "    # Prepare the results in the expected format\n",
    "    documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"ground_truths\": doc['ground_truths'], \n",
    "          \"query\": doc['query']}   \n",
    "         for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "    logger.debug(\"Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return validation_examples\n",
    "\n",
    "validation_examples = generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_results(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()  # Read the entire content as a string\n",
    "        baseline_results = json.loads(content)\n",
    "\n",
    "    # Flatten the list of results and rename to database_result\n",
    "    database_result = [item for sublist in baseline_results for item in sublist]\n",
    "\n",
    "    # Initialize lists to store the values\n",
    "    queries = []  # List for queries\n",
    "    answer = []  # List for predicted answers\n",
    "    context = []  # List for context\n",
    "    ground_truths = []  # List for ground truth answers\n",
    "\n",
    "    # Populate the lists\n",
    "    for i, eg in enumerate(database_result):\n",
    "        queries.append(database_result[i]['query'])\n",
    "        answer.append(database_result[i]['answer'])\n",
    "        context.append([database_result[i]['context']])\n",
    "        ground_truths.append(database_result[i]['answer'])\n",
    "\n",
    "    # Create the final dictionary\n",
    "    final_dict = {\n",
    "        \"question\": queries,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": context,\n",
    "        \"ground_truth\": ground_truths\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a Dataset\n",
    "    Dataset_results = Dataset.from_dict(final_dict)\n",
    "\n",
    "    return Dataset_results\n",
    "\n",
    "# Example usage:\n",
    "file_path_baseline = \"results/query_OpenAI_baselinemethod_documents_dict_results.json\"\n",
    "RAG_baseline_results = validate_results(file_path_baseline)\n",
    "\n",
    "file_path_contextualized = \"results/query_OpenAI_contextualize_query_with_categories_dict_results.json\"\n",
    "RAG_contextualized_results = validate_results(file_path_baseline)\n",
    "\n",
    "file_path_hierarchical = \"results/query_OpenAI_hierarchicalmethod_documents_dict_results.json\"\n",
    "RAG_hierarchical_results = validate_results(file_path_hierarchical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac724a7523b345ff9fcad322b65ad163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa13cc1fc744f388d2e1a89c68a2d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness, context_precision, context_recall, context_entity_recall\n",
    "\n",
    "RAG_baseline_results_metrics = evaluate(RAG_baseline_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_baseline_results_df = RAG_baseline_results_metrics.to_pandas()\n",
    "RAG_baseline_results_df['method']= 'baseline'\n",
    "\n",
    "RAG_contextualized_results_metrics = evaluate(RAG_contextualized_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_contextualized_results_df = RAG_contextualized_results_metrics.to_pandas()\n",
    "RAG_contextualized_results_df['method']= 'Contextualized'\n",
    "\n",
    "\n",
    "RAG_hierarchical_results_metrics = evaluate(RAG_hierarchical_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_hierarchical_results_df = RAG_hierarchical_results_metrics.to_pandas()\n",
    "RAG_hierarchical_results_df['method']= 'hierarchical'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to {results_filename}\n"
     ]
    }
   ],
   "source": [
    "comparing_results = pd.concat([RAG_baseline_results_df, RAG_contextualized_results_df, RAG_hierarchical_results_df],axis=0)\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['method']], axis=1)\n",
    "\n",
    "mean_result_by_method = numeric_comparing_results.groupby(['method'])\\\n",
    "                                                .agg('mean').reset_index()\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_filename = f\"results/comparing_methods_meanresults.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mean_result_by_method.to_csv(results_filename, index=False)  # `index=False` prevents writing row numbers\n",
    "\n",
    "print(\"Documents have been saved to {results_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to {results_filename}\n"
     ]
    }
   ],
   "source": [
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['user_input']], axis=1)\n",
    "\n",
    "mean_result_by_method_userinput = numeric_comparing_results.groupby(['user_input'])\\\n",
    "                                                           .agg('mean').reset_index()   \n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_filename = f\"results/comparing_methods_meanresults_byinputs.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mean_result_by_method_userinput.to_csv(results_filename, index=False)  # `index=False` prevents writing row numbers\n",
    "\n",
    "print(\"Documents have been saved to {results_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-chat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
