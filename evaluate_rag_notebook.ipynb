{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importando as bibliotecas necessárias "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness, context_precision, context_recall, context_entity_recall\n",
    "from functions import load_VectorStore, generate_and_save_validation_examples, validate_results, compare_time_methods,  baseline_similarity_method, contextualize_query_with_categories, hierarchical_retrieval_with_categories\n",
    "\n",
    "# Configurar o logging para salvar a saída de debug em um arquivo\n",
    "logging.basicConfig(\n",
    "    filename='./logs/debug_output.log',  # O arquivo onde os logs serão salvos\n",
    "    level=logging.DEBUG,          # O nível de log (DEBUG para capturar tudo)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Formato do log\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o documento e criando o split em chuncks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # read local .env file\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'inputs/OutdoorClothingCatalog_1000_withCategories.csv'\n",
    "loader = CSVLoader(file_path=file, encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "\n",
    "split_documents = []\n",
    "for doc in docs:\n",
    "    split_docs = text_splitter.split_documents([doc])\n",
    "    split_documents.extend(split_docs)\n",
    "    \n",
    "documents_dict = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in split_documents\n",
    "]\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open(\"inputs/documents_split_langchain.json\", \"w\") as file:\n",
    "    json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "logger.debug(\"Documents have been saved to 'documents_split_langchain.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents split\n",
    "with open(\"inputs/documents_split_langchain.json\", \"r\") as file:\n",
    "    documents_dict = json.load(file)\n",
    "\n",
    "# Convert the list of dictionaries back to a list of Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n",
    "    for doc in documents_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando os embedding via OpenAIEmbeddings e criando um vector Store via Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando o Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating embeddings \n",
    "# openai_embedding_function(documents, model = \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_provider = 'OpenAI'\n",
    "vector_store = load_VectorStore(embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as querys para validar os métodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\llm.py:369: UserWarning: The apply_and_parse method is deprecated, instead pass an output parser directly to LLMChain.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'context': Document(metadata={'source': 'inputs/OutdoorClothingCatalog_1000_withCategories.csv', 'row': 338}, page_content='Unnamed: 0: 363\\nname: Men\\'s Leather Lace-Ups by ® 8\"\\ndescription: Our legendary Maine-made  Boot – designed by \"L.L.\" himself in 1912 and keeping feet dry and comfortable ever since. Chances are, you’ll only ever need one pair. \\r\\n\\r\\nSize & Fit\\r\\nWith light- or midweight socks: Whole sizes, order 1 size down. Half sizes, order 1½ sizes down.\\r\\nWith heavyweight socks: Whole sizes, order your normal size. Half sizes, order the next size down.\\r\\n\\r\\nWhy We Love Them\\r\\nWhoever says “they don’t build things like they used to,” doesn\\'t own these boots. Today, our signature boots are still sewn right here in Maine – one pair at a time – by expert craftspeople whose technical skills and passion for their work is evident in every pair of boots they make. Warm, dry feet haven’t gone out of style in a hundred years, so we haven’t needed to change L.L.’s innovative design.\\r\\n\\r\\nVideo: One Minute on  Boot Care\\r\\n\\r\\nConstruction\\r\\nWaterproof protection of rubber-bottom boots with the supple comfort of full-grain leather. A uniquely shaped foot form offers\\nclassified_products: [\\'Footwear\\', \\'Boots\\', \"Men\\'s Clothing\"]'),\n",
       "  'query': \"What are the sizing recommendations for the Men's Leather Lace-Ups when wearing different types of socks, according to the document?\",\n",
       "  'ground_truths': \"For the Men's Leather Lace-Ups, the sizing recommendations are as follows: \"},\n",
       " {'context': Document(metadata={'source': 'inputs/OutdoorClothingCatalog_1000_withCategories.csv', 'row': 744}, page_content='Unnamed: 0: 779\\nname: Comfy Knee-Highs, Two-Pack\\ndescription: Our crew-height ragg socks are the perfect combination of comfort and lightweight design. Soft to the touch and itch-free, these socks are great for wearing with wellies or rubber mocs. With 63% cotton, 22% polyester, 14% nylon, and 1% spandex, these socks are sure to fit as comfortably as they look. The diamond cable pattern and solid toe ensure a secure fit. Plus, these socks are proudly made in the USA. Care for them is easy – machine wash and dry. So, get ready to enjoy them during the warm spring and summer months!\\nclassified_products: [\\'Clothing\\', \\'Socks\\', \"Men\\'s Accessories\"]'),\n",
       "  'query': 'What are the key features and materials used in the Comfy Knee-Highs, Two-Pack socks, and how should they be cared for?',\n",
       "  'ground_truths': 'The Comfy Knee-Highs, Two-Pack socks are made from a blend of 63% cotton, 22% polyester, 14% nylon, and 1% spandex, providing a soft, lightweight, and itch-free wearing experience. They feature a diamond cable pattern and a solid toe for a secure fit, making them suitable for wearing with wellies or rubber mocs. These socks are proudly made in the USA and are easy to care for, as they can be machine washed and dried.'},\n",
       " {'context': Document(metadata={'source': 'inputs/OutdoorClothingCatalog_1000_withCategories.csv', 'row': 606}, page_content='Unnamed: 0: 641\\nname: Women\\'s Featherlight 850 Down Jacket, Print\\ndescription: Our revolutionary 850 down jacket defies cold, wet weather – and comparison. We combined all the best outerwear technology – innovative DownTek fill, 850-fill-power and a Pertex Quantum polyester shell – for unbeatable performance. When it comes to comfort ratings, this jacket can handle light activity up to 30° and moderate activity up to -20°. Its slightly fitted cut with a slightly slimmer waist is best with midweight layers and falls at hip. \\r\\n\\r\\nWe start with premium 850 down, which provides more warmth for its weight, and then treat it with revolutionary DownTek that allows our jackets to do what many others can’t – keep you warm, even in wet conditions. Unlike ordinary down that soaks up moisture, ours absorbs 33% less moisture and dries 66% faster so it stays light, lofty and warm. On the outside, the game-changing Pertex Quantum nylon shell is tightly woven to resist the elements.\\r\\n\\r\\nThe 850-fill-power goose down is one of the highest on the market, delivering more warmth for weight. Additionally, the premium Pertex Quantum nylon shell is tightly woven for wind and weather-resistance. Certified by the Responsible Down Standard, this jacket ensures the\\nclassified_products: [\"Women\\'s Clothing\", \\'Outerwear\\', \\'Jackets\\']'),\n",
       "  'query': \"What are the key features and technologies that make the Women's Featherlight 850 Down Jacket effective for cold and wet weather conditions?\",\n",
       "  'ground_truths': \"The Women's Featherlight 850 Down Jacket features innovative technologies such as DownTek fill, which allows the jacket to retain warmth even in wet conditions by absorbing 33% less moisture and drying 66% faster than ordinary down. It has a high 850-fill-power goose down, providing exceptional warmth for its weight. The jacket is also equipped with a Pertex Quantum polyester shell that is tightly woven to resist wind and weather, enhancing its overall performance. The jacket's comfort ratings indicate it can handle light activity in temperatures up to 30° and moderate activity in temperatures down to -20°. Additionally, it has a slightly fitted cut with a slimmer waist, designed to be worn over midweight layers, and falls at the hip. It is also certified by the Responsible Down Standard, ensuring ethical sourcing of down materials.\"},\n",
       " {'context': Document(metadata={'source': 'inputs/OutdoorClothingCatalog_1000_withCategories.csv', 'row': 911}, page_content='Unnamed: 0: 948\\nname: Rain Shield Outdoor Blanket\\ndescription: Don\\'t let a little wet grass ruin your picnic or breakfast at the campsite. This versatile waterproof blanket has a rugged polyurethane-coated nylon backing that keeps moisture from seeping through to the supersoft fleece surface. \\r\\n\\r\\nSpecs \\r\\nDimensions: 72\" x 58\".\\r\\nFabric & Care \\r\\nTop: 100% polyester fleece. \\r\\nBottom: 100% nylon.\\r\\nMachine wash and dry. \\r\\nConstruction \\r\\nSuper-soft polyester-fleece surface. \\r\\nRugged polyurethane-coated nylon backing keeps moisture from seeping through. \\r\\nAdditional Features \\r\\nGreat choice for sporting events, the beach on a boat or in your car\\'s safety kit. \\r\\nStuff sack included. \\r\\nImported. \\r\\n\\r\\n.hr-p6 {         color: #717171;         margin: 20px 0;     }     .dtb-p6 {         color: #464749;         font-size: 14px;         font-family: Arial, Helvetica, sans-serif;         width: 70%;          display: block;          float: left;     }     .dt\\nclassified_products: [\\'Outdoor Gear\\', \\'Camping Accessories\\', \\'Blankets\\']'),\n",
       "  'query': 'What are the key features and specifications of the Rain Shield Outdoor Blanket, and what materials are used in its construction?',\n",
       "  'ground_truths': 'The Rain Shield Outdoor Blanket has a versatile waterproof design with a rugged polyurethane-coated nylon backing that prevents moisture from seeping through to its supersoft fleece surface. The blanket measures 72\" x 58\" and is made from 100% polyester fleece on the top and 100% nylon on the bottom. It is machine washable and can be dried in a machine. Additional features include its suitability for various outdoor activities such as sporting events, the beach, or as part of a car\\'s safety kit, and it comes with a stuff sack for easy transport. The product is imported and classified under outdoor gear, camping accessories, and blankets.'},\n",
       " {'context': Document(metadata={'source': 'inputs/OutdoorClothingCatalog_1000_withCategories.csv', 'row': 619}, page_content='Unnamed: 0: 652\\nname: Kids\\' Adventure Rider Bike, 20\"\\ndescription: This mountain bike is easy-to-ride and offers great durability and stability for the young rider who is gaining skills and confidence. It fits users 38\" to 54\" and features a tire size of 20\" diam. x 2.0\"W. The mountain-bike frame has a low stand-over height for control and stability, and wide, knobby tires provide extra grip on the road or trail. It comes with a 7-Speed Shimano drivetrain with an easy-to-use SRAM shifter, and the rugged steel frame and fork is designed to last for years. It also includes adjustable front and rear V-Brakes, a kickstand, and an adjustable seat height. Assembly instructions are available in PDF form.\\nclassified_products: [\"Kids\\' Clothing\", \\'Outdoor Gear\\', \\'Bicycles\\']'),\n",
       "  'query': \"What are the key features of the Kids' Adventure Rider Bike, and what is the recommended height range for users?\",\n",
       "  'ground_truths': 'The Kids\\' Adventure Rider Bike features a mountain-bike frame with a low stand-over height for control and stability, wide knobby tires for extra grip, a 7-Speed Shimano drivetrain with an easy-to-use SRAM shifter, adjustable front and rear V-Brakes, a kickstand, and an adjustable seat height. It is designed for users who are between 38\" to 54\" tall.'}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_examples = generate_and_save_validation_examples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando métodos de RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline - Técnica por similaridade entre query e documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_similarity_method(validation_examples, vector_store, embeddings_provider):\n",
    "    \"\"\"\n",
    "    Implements a baseline retrieval-augmented generation (RAG) method to answer queries \n",
    "    using a similarity-based retriever and a language model.\n",
    "\n",
    "    Args:\n",
    "        validation_examples (list): A list of validation examples to query the model.\n",
    "        vector_store (object): A vector store instance for similarity-based retrieval.\n",
    "        embeddings_provider (str): The name of the embeddings provider (used in result file naming).\n",
    "\n",
    "    Returns:\n",
    "        list: A dictionary containing the retrieved contexts, metadata, queries, \n",
    "              ground truths, and generated answers for each validation example.\n",
    "\n",
    "    \"\"\"\n",
    "    logger.debug(\"Baseline similarity method to answer queries initiated.\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    model = 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "    # Initialize the language model\n",
    "    llm = OpenAI(temperature=0.2, model=model)\n",
    "\n",
    "    # Set up the retriever with similarity search\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=5)  # Retrieve the most relevant chunks\n",
    "\n",
    "    # Initialize the RetrievalQA chain\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\"document_separator\": \"\\n\"}\n",
    "    )\n",
    "\n",
    "    # Apply the model to the validation examples\n",
    "    predictions = qa.apply(validation_examples)\n",
    "\n",
    "    # Prepare the results\n",
    "    baselinemethod_documents_dict = [\n",
    "        [\n",
    "            {\n",
    "                \"context\": doc['context'].page_content,\n",
    "                \"metadata\": doc['context'].metadata,\n",
    "                \"query\": doc['query'],\n",
    "                \"ground_truths\": doc['ground_truths'],\n",
    "                \"answer\": doc['result']\n",
    "            }\n",
    "            for doc in data\n",
    "        ]\n",
    "        for data in [predictions]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_baselinemethod_documents_dict_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(baselinemethod_documents_dict, file, indent=4)\n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    logger.debug(\"Baseline similarity method documents have been saved to %s\", results_filename)\n",
    "\n",
    "    Result_time = end_time - start_time\n",
    "\n",
    "    return Result_time, baselinemethod_documents_dict\n",
    "\n",
    "baselinetime, baselinemethod_documents_dict = baseline_similarity_method(validation_examples, vector_store, embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnica por similaridade com adição de contextualização incluindo na query categorias referentes a pergunta para a busca textual \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contextualize_query_with_categories(vector_store, validation_examples, embeddings_provider):\n",
    "    \"\"\"\n",
    "    Enhances queries by contextualizing them with relevant categories before performing a document search and generating answers.\n",
    "\n",
    "    Args:\n",
    "        vector_store (object): The vector store used for similarity-based retrieval of documents.\n",
    "        validation_examples (list): A list of validation examples containing queries and expected ground truths.\n",
    "        embeddings_provider (str): The name of the embeddings provider, used in naming the results file.\n",
    "\n",
    "    Returns:\n",
    "        list: A dictionary containing the contextualized queries, retrieved contexts, metadata, ground truths, \n",
    "              and generated answers for each validation example.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Contextualized query method to answer queries initiated.\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define the high-level query template for category search\n",
    "    category_prompt = \"\"\"\n",
    "    You are an expert in outdoor clothing products. Based on the following query, \n",
    "    return a list of relevant categories (e.g., jackets, pants) from the catalog.\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    category_template = PromptTemplate(input_variables=[\"query\"], template=category_prompt)\n",
    "    category_chain = LLMChain(prompt=category_template, llm=OpenAI())\n",
    "\n",
    "    # Define the refined search for documents within each category\n",
    "    document_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "    Your function is to use the catalog of retrieved context to answer the client's questions based on the following query and the category {category}.\n",
    "    Query: {query}\n",
    "    Category: {category}\n",
    "    Catalog: {catalog}\n",
    "    \"\"\"\n",
    "    document_template = PromptTemplate(input_variables=[\"query\", \"category\", \"catalog\"], template=document_prompt)\n",
    "    document_chain = LLMChain(prompt=document_template, llm=OpenAI())\n",
    "\n",
    "    for index, item in enumerate(validation_examples):\n",
    "        query = item['query']\n",
    "        \n",
    "        # Obtain the relevant category for the query\n",
    "        category_result = category_chain.run({\"query\": query})\n",
    "        logger.debug(\"Category identified for query '%s': %s\", query, category_result)\n",
    "\n",
    "        # Combine query and category for refined search\n",
    "        modified_query = f\"{query} Category: {category_result}\"\n",
    "        catalog = vector_store.similarity_search(modified_query, k=5)  # Retrieve relevant catalog entries\n",
    "\n",
    "        # Generate refined answer using retrieved catalog\n",
    "        refined_answer = document_chain.run({\n",
    "            \"query\": query,\n",
    "            \"category\": category_result,\n",
    "            \"catalog\": catalog\n",
    "        })\n",
    "        validation_examples[index]['result'] = refined_answer\n",
    "\n",
    "    # Prepare the results in dictionary format\n",
    "    contextualize_query_with_categories_dict = [\n",
    "        [\n",
    "            {\n",
    "                \"context\": doc['context'].page_content,\n",
    "                \"metadata\": doc['context'].metadata,\n",
    "                \"query\": doc['query'],\n",
    "                \"ground_truths\": doc['ground_truths'],\n",
    "                \"answer\": doc['result'],\n",
    "            }\n",
    "            for doc in data\n",
    "        ]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_contextualize_query_with_categories_dict_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(contextualize_query_with_categories_dict, file, indent=4)\n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(\"Contextualized query method documents have been saved to %s\", results_filename)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    Result_time = end_time - start_time\n",
    "\n",
    "    return Result_time, contextualize_query_with_categories_dict\n",
    "\n",
    "# Assuming vector_store is defined as per the previous setup\n",
    "contextualizetime, contextualize_query_with_categories_dict = contextualize_query_with_categories(vector_store, validation_examples,embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnica por similaridade incluindo filtro de documentos buscando similaridade entre categorias e documentos para a busca textual "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_retrieval_with_categories(vector_store, validation_examples, embeddings_provider):\n",
    "    \"\"\"\n",
    "    Implements a hierarchical retrieval method to answer queries by categorizing and refining the search process.\n",
    "\n",
    "    This method uses a two-step approach:\n",
    "    1. Identifies relevant categories based on the input query using a language model.\n",
    "    2. Retrieves relevant documents within each identified category and refines the answer using another language model.\n",
    "\n",
    "    Parameters:\n",
    "        vector_store (VectorStore): The vector store for similarity-based document retrieval.\n",
    "        validation_examples (list): List of validation examples, each containing a query and related information.\n",
    "        embeddings_provider (str): The name of the embeddings provider used for vectorizing the documents.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing context, metadata, query, ground truths, and answers.\n",
    "    \"\"\"\n",
    "    logger.debug(\"Hierarchical retrieval method to answer queries initiated\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Define the high-level query template for category search\n",
    "    category_prompt = \"\"\"\n",
    "    You are an expert in outdoor clothing products. Based on the following query, \n",
    "    return a list of relevant categories (e.g., jackets, pants) from the catalog.\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    category_template = PromptTemplate(input_variables=[\"query\"], template=category_prompt)\n",
    "    category_chain = LLMChain(prompt=category_template, llm=OpenAI())\n",
    "\n",
    "    # Define the refined search for documents within each category\n",
    "    document_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "    Your function is to use the catalog of retrieved context to answer the client's questions based on the following query.\n",
    "    Query: {query}\n",
    "    Catalog: {catalog}\n",
    "    \"\"\"\n",
    "    document_template = PromptTemplate(input_variables=[\"query\", \"catalog\"], template=document_prompt)\n",
    "    document_chain = LLMChain(prompt=document_template, llm=OpenAI())\n",
    "\n",
    "    for index, item in enumerate(validation_examples):\n",
    "        query = item['query']\n",
    "        category_result = category_chain.run({\"query\": query})\n",
    "    \n",
    "        # Convert category_result to vector (embedding) using the same model as the vector store\n",
    "        relevant_documents = vector_store.similarity_search(category_result, k=5)\n",
    "\n",
    "        if not relevant_documents:\n",
    "            logger.debug(f\"No relevant documents found for category '{category_result}'\")  \n",
    "            continue\n",
    "        \n",
    "        # Refine the answer using the filtered documents\n",
    "        refined_answer = document_chain.run({\"query\": query, \"catalog\": relevant_documents})\n",
    "        validation_examples[index]['result'] = refined_answer\n",
    "\n",
    "    hierarchicalmethod_documents_dict = [\n",
    "        [\n",
    "            {\n",
    "                \"context\": doc['context'].page_content, \n",
    "                \"metadata\": doc['context'].metadata,\n",
    "                \"query\": doc['query'],\n",
    "                \"ground_truths\": doc['ground_truths'],  \n",
    "                \"answer\": doc['result'],\n",
    "            }\n",
    "            for doc in data\n",
    "        ]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the list of dictionaries to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_hierarchicalmethod_documents_dict_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(hierarchicalmethod_documents_dict, file, indent=4)\n",
    "        \n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(f\"Hierarchical method Documents have been saved to {results_filename}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    Result_time = end_time - start_time\n",
    "    \n",
    "    return Result_time, hierarchicalmethod_documents_dict\n",
    "\n",
    "# Assuming vector_store is defined as per the previous setup\n",
    "hierarchicaltime, hierarchicalmethod_documents_dict = hierarchical_retrieval_with_categories(vector_store, validation_examples,embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Técnica de Recuperação-Para-Geração (RAG) com dois estágios\n",
    "\n",
    "###  classificação de relevância de documentos e resposta gerada baseada nos documentos relevantes usando o ChatGroq \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "# from langchain_groq import ChatGroq\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# # Data model\n",
    "# class GradeDocuments(BaseModel):\n",
    "#     \"\"\"Ordinal score for relevance check on retrieved documents.\"\"\"\n",
    "#     OrdinalScore: int = Field(\n",
    "#         description=\"Score between 1 (low) and 5 (high)\"\n",
    "#     )\n",
    "\n",
    "# # Function to grade a document\n",
    "# def grade_document(doc, question, llm, grade_prompt):\n",
    "#     res = grade_prompt.invoke({\"question\": question, \"document\": doc.page_content})\n",
    "#     return doc, res\n",
    "\n",
    "# # Function to grade and retrieve relevant documents in parallel\n",
    "# def grade_and_retrieve_documents(docs, question):\n",
    "#     # LLM with function call\n",
    "#     llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "#     structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "#     # Prompt for grading documents\n",
    "#     system = \"\"\"You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "#     Your function is to use the catalog of retrieved context to answer the client's questions based on the question.\n",
    "#     Give a OrdinalScore score '1 (low) and 5 (high)' to indicate whether the document is relevant to the question.\"\"\"\n",
    "\n",
    "#     grade_prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system),\n",
    "#             (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     retrieval_grader = grade_prompt | structured_llm_grader\n",
    "\n",
    "#     # Use ThreadPoolExecutor to parallelize document grading\n",
    "#     docs_to_use = []\n",
    "#     with ThreadPoolExecutor() as executor:\n",
    "#         futures = [executor.submit(grade_document, doc, question, llm, retrieval_grader) for doc in docs]\n",
    "#         for future in as_completed(futures):\n",
    "#             doc, res = future.result()\n",
    "#             if res.OrdinalScore > 3:  # Keep documents with score greater than 3\n",
    "#                 docs_to_use.append(doc)\n",
    "\n",
    "#     return docs_to_use\n",
    "\n",
    "# # Function to format documents for the next step\n",
    "# def format_docs(docs):\n",
    "#     return \"\\n\".join(f\"<doc{i+1}>:\\nTitle:{doc.metadata['title']}\\nSource:{doc.metadata['source']}\\nContent:{doc.page_content}\\n</doc{i+1}>\\n\" for i, doc in enumerate(docs))\n",
    "\n",
    "# # Function to generate the final response using the relevant documents\n",
    "# def generate_answer(docs_to_use, question):\n",
    "#     # Prompt for answering the question based on relevant documents\n",
    "#     system = \"\"\"You are an assistant for question-answering tasks. Answer the question based upon your knowledge. \n",
    "#     Use three-to-five sentences maximum and keep the answer concise.\"\"\"\n",
    "\n",
    "#     prompt = ChatPromptTemplate.from_messages(\n",
    "#         [\n",
    "#             (\"system\", system),\n",
    "#             (\"human\", \"Retrieved documents: \\n\\n <docs>{documents}</docs> \\n\\n User question: <question>{question}</question>\"),\n",
    "#         ]\n",
    "#     )\n",
    "\n",
    "#     # LLM for answering the question\n",
    "#     llm = ChatGroq(model=\"llama-3.1-8b-instant\", temperature=0)\n",
    "\n",
    "#     # Post-processing to parse the output\n",
    "#     rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "#     # Format documents for input\n",
    "#     formatted_docs = format_docs(docs_to_use)\n",
    "\n",
    "#     # Generate the answer\n",
    "#     generation = rag_chain.invoke({\"documents\": formatted_docs, \"question\": question})\n",
    "#     print(generation)\n",
    "\n",
    "# # Main function to run the entire flow\n",
    "# def main(docs, question):\n",
    "#     # Step 1: Grade and retrieve relevant documents\n",
    "#     docs_to_use = grade_and_retrieve_documents(docs, question)\n",
    "    \n",
    "#     # Step 2: Generate the final answer using the relevant documents\n",
    "#     generate_answer(docs_to_use, question)\n",
    "\n",
    "# # Example usage\n",
    "# question = 'Product with sun protection'\n",
    "\n",
    "# # Run the main function\n",
    "# main(docs, question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embeddings_provider = 'OpenAI'\n",
    "vector_store = load_VectorStore(embeddings_provider)\n",
    "\n",
    "\n",
    "embeddings_provider = 'OpenAI'\n",
    "vector_store = load_VectorStore(embeddings_provider)\n",
    "with open(\"results/query_OpenAI_results.json\", \"r\") as file:\n",
    "        content = file.read()  # Read the entire content as a string\n",
    "        database_result = json.loads(content)\n",
    "\n",
    "validation_examples = [item for sublist in database_result for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "argument 'text': 'list' object cannot be converted to 'PyString'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m         content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()  \u001b[38;5;66;03m# Read the entire content as a string\u001b[39;00m\n\u001b[0;32m      5\u001b[0m         validation_examples \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(content)\n\u001b[1;32m----> 8\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_time_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_provider\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_similarity_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontextualize_query_with_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhierarchical_retrieval_with_categories\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\functions.py:624\u001b[0m, in \u001b[0;36mcompare_time_methods\u001b[1;34m(validation_examples, vector_store, embeddings_provider, baseline_similarity_method, contextualize_query_with_categories, hierarchical_retrieval_with_categories)\u001b[0m\n\u001b[0;32m    611\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    612\u001b[0m \u001b[38;5;124;03mCompare the performance of baseline and generative retrieval methods.\u001b[39;00m\n\u001b[0;32m    613\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    621\u001b[0m \u001b[38;5;124;03m    dict: A dictionary containing the comparison of time taken and precision for both methods.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    623\u001b[0m \u001b[38;5;66;03m# Baseline method evaluation\u001b[39;00m\n\u001b[1;32m--> 624\u001b[0m baseline_time \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbaseline_similarity_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_examples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_provider\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    625\u001b[0m contextualize_time \u001b[38;5;241m=\u001b[39m evaluate_time(contextualize_query_with_categories,validation_examples, vector_store, embeddings_provider)\n\u001b[0;32m    626\u001b[0m hierarchical_time \u001b[38;5;241m=\u001b[39m evaluate_time(hierarchical_retrieval_with_categories,validation_examples, vector_store, embeddings_provider)\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\functions.py:606\u001b[0m, in \u001b[0;36mevaluate_time\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_time\u001b[39m(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    595\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;124;03m    Measure the execution time of a function without returning its result.\u001b[39;00m\n\u001b[0;32m    597\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    604\u001b[0m \u001b[38;5;124;03m        float: The time taken by the function (in seconds).\u001b[39;00m\n\u001b[0;32m    605\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 606\u001b[0m     _, time_taken \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    607\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m time_taken\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\functions.py:386\u001b[0m, in \u001b[0;36mmeasure_time\u001b[1;34m(func, *args, **kwargs)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03mMeasure the execution time of a given function.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;124;03m    tuple: A tuple containing the function result and the time taken (in seconds).\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    385\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m--> 386\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    387\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result, end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\functions.py:342\u001b[0m, in \u001b[0;36mbaseline_similarity_method\u001b[1;34m(validation_examples, vector_store, embeddings_provider)\u001b[0m\n\u001b[0;32m    333\u001b[0m qa \u001b[38;5;241m=\u001b[39m RetrievalQA\u001b[38;5;241m.\u001b[39mfrom_chain_type(\n\u001b[0;32m    334\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[0;32m    335\u001b[0m     chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstuff\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    338\u001b[0m     chain_type_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_separator\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m    339\u001b[0m )\n\u001b[0;32m    341\u001b[0m \u001b[38;5;66;03m# Apply the model to the validation examples\u001b[39;00m\n\u001b[1;32m--> 342\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mqa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalidation_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# Prepare the results\u001b[39;00m\n\u001b[0;32m    345\u001b[0m baselinemethod_documents_dict \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    346\u001b[0m     [\n\u001b[0;32m    347\u001b[0m         {\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m [predictions]\n\u001b[0;32m    357\u001b[0m ]\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\base.py:769\u001b[0m, in \u001b[0;36mChain.apply\u001b[1;34m(self, input_list, callbacks)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28mself\u001b[39m, input_list: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the chain on all inputs in the list.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\base.py:769\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;129m@deprecated\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m, alternative\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, removal\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m    766\u001b[0m     \u001b[38;5;28mself\u001b[39m, input_list: List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]], callbacks: Callbacks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    767\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[0;32m    768\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the chain on all inputs in the list.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m inputs \u001b[38;5;129;01min\u001b[39;00m input_list]\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\base.py:389\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[0;32m    381\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    382\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[0;32m    384\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[0;32m    386\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[0;32m    387\u001b[0m }\n\u001b[1;32m--> 389\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    394\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[1;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[0;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\base.py:160\u001b[0m, in \u001b[0;36mChain.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[0;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 160\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[0;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[0;32m    163\u001b[0m     )\n\u001b[0;32m    165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[0;32m    166\u001b[0m         inputs, outputs, return_only_outputs\n\u001b[0;32m    167\u001b[0m     )\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:151\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[1;34m(self, inputs, run_manager)\u001b[0m\n\u001b[0;32m    147\u001b[0m accepts_run_manager \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs)\u001b[38;5;241m.\u001b[39mparameters\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accepts_run_manager:\n\u001b[1;32m--> 151\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    153\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_docs(question)  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain\\chains\\retrieval_qa\\base.py:271\u001b[0m, in \u001b[0;36mRetrievalQA._get_docs\u001b[1;34m(self, question, run_manager)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_docs\u001b[39m(\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    266\u001b[0m     question: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[0;32m    268\u001b[0m     run_manager: CallbackManagerForChainRun,\n\u001b[0;32m    269\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Get docs.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 271\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\retrievers.py:254\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    253\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_error(e)\n\u001b[1;32m--> 254\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    256\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_retriever_end(\n\u001b[0;32m    257\u001b[0m         result,\n\u001b[0;32m    258\u001b[0m     )\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\retrievers.py:247\u001b[0m, in \u001b[0;36mBaseRetriever.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    245\u001b[0m _kwargs \u001b[38;5;241m=\u001b[39m kwargs \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expects_other_args \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_arg_supported:\n\u001b[1;32m--> 247\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_relevant_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\n\u001b[0;32m    249\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    251\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_relevant_documents(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs)\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:1080\u001b[0m, in \u001b[0;36mVectorStoreRetriever._get_relevant_documents\u001b[1;34m(self, query, run_manager)\u001b[0m\n\u001b[0;32m   1076\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_relevant_documents\u001b[39m(\n\u001b[0;32m   1077\u001b[0m     \u001b[38;5;28mself\u001b[39m, query: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m, run_manager: CallbackManagerForRetrieverRun\n\u001b[0;32m   1078\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1080\u001b[0m         docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvectorstore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1081\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimilarity_score_threshold\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   1082\u001b[0m         docs_and_similarities \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvectorstore\u001b[38;5;241m.\u001b[39msimilarity_search_with_relevance_scores(\n\u001b[0;32m   1084\u001b[0m                 query, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msearch_kwargs\n\u001b[0;32m   1085\u001b[0m             )\n\u001b[0;32m   1086\u001b[0m         )\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:582\u001b[0m, in \u001b[0;36mChroma.similarity_search\u001b[1;34m(self, query, k, filter, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    566\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    570\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[0;32m    571\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Run similarity search with Chroma.\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \n\u001b[0;32m    573\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;124;03m        List of documents most similar to the query text.\u001b[39;00m\n\u001b[0;32m    581\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 582\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_chroma\\vectorstores.py:679\u001b[0m, in \u001b[0;36mChroma.similarity_search_with_score\u001b[1;34m(self, query, k, filter, where_document, **kwargs)\u001b[0m\n\u001b[0;32m    671\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    672\u001b[0m         query_texts\u001b[38;5;241m=\u001b[39m[query],\n\u001b[0;32m    673\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    677\u001b[0m     )\n\u001b[0;32m    678\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 679\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    680\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__query_collection(\n\u001b[0;32m    681\u001b[0m         query_embeddings\u001b[38;5;241m=\u001b[39m[query_embedding],\n\u001b[0;32m    682\u001b[0m         n_results\u001b[38;5;241m=\u001b[39mk,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    688\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _results_to_docs_and_scores(results)\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:629\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_query\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    620\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[0;32m    621\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\u001b[39;00m\n\u001b[0;32m    622\u001b[0m \n\u001b[0;32m    623\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;124;03m        Embedding for the text.\u001b[39;00m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:588\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001b[39;00m\n\u001b[0;32m    586\u001b[0m \u001b[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001b[39;00m\n\u001b[0;32m    587\u001b[0m engine \u001b[38;5;241m=\u001b[39m cast(\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeployment)\n\u001b[1;32m--> 588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:480\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[1;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;124;03mGenerate length-safe embeddings for a list of texts.\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;124;03m    List[List[float]]: A list of embeddings for each input text.\u001b[39;00m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    479\u001b[0m _chunk_size \u001b[38;5;241m=\u001b[39m chunk_size \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size\n\u001b[1;32m--> 480\u001b[0m _iter, tokens, indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_chunk_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    481\u001b[0m batched_embeddings: List[List[\u001b[38;5;28mfloat\u001b[39m]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m _iter:\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:441\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._tokenize\u001b[1;34m(self, texts, chunk_size)\u001b[0m\n\u001b[0;32m    439\u001b[0m     token \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mencode(text, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mencoder_kwargs)\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 441\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[43mencoding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_ordinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;66;03m# Split tokens into chunks respecting the embedding_ctx_length\u001b[39;00m\n\u001b[0;32m    444\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length):\n",
      "File \u001b[1;32md:\\Repos-foradrive\\Estudos\\desafio-tecnico-ds-ai-chat\\ds-ai-chat-env\\Lib\\site-packages\\tiktoken\\core.py:69\u001b[0m, in \u001b[0;36mEncoding.encode_ordinary\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Encodes a string into tokens, ignoring special tokens.\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \u001b[38;5;124;03mThis is equivalent to `encode(text, disallowed_special=())` (but slightly faster).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;124;03m[31373, 995]\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_core_bpe\u001b[38;5;241m.\u001b[39mencode_ordinary(text)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;66;03m# See comment in encode\u001b[39;00m\n\u001b[0;32m     72\u001b[0m     text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msurrogatepass\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-16\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: argument 'text': 'list' object cannot be converted to 'PyString'"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "file_path_baseline = \"results/query_OpenAI_baselinemethod_documents_dict_results.json\"\n",
    "RAG_baseline_results = validate_results(file_path_baseline)\n",
    "\n",
    "file_path_contextualized = \"results/query_OpenAI_contextualize_query_with_categories_dict_results.json\"\n",
    "RAG_contextualized_results = validate_results(file_path_baseline)\n",
    "\n",
    "file_path_hierarchical = \"results/query_OpenAI_hierarchicalmethod_documents_dict_results.json\"\n",
    "RAG_hierarchical_results = validate_results(file_path_hierarchical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d2bfa72eada4485a0104d6bd2e08358",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e34b4effa38478ea04b6b3a73c001ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26205dd3186a4f77a2b203302c2e9143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "RAG_baseline_results_metrics = evaluate(RAG_baseline_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_baseline_results_df = RAG_baseline_results_metrics.to_pandas()\n",
    "RAG_baseline_results_df['method']= 'baseline'\n",
    "\n",
    "RAG_contextualized_results_metrics = evaluate(RAG_contextualized_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_contextualized_results_df = RAG_contextualized_results_metrics.to_pandas()\n",
    "RAG_contextualized_results_df['method']= 'Contextualized'\n",
    "\n",
    "\n",
    "RAG_hierarchical_results_metrics = evaluate(RAG_hierarchical_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_hierarchical_results_df = RAG_hierarchical_results_metrics.to_pandas()\n",
    "RAG_hierarchical_results_df['method']= 'hierarchical'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to {results_filename}\n"
     ]
    }
   ],
   "source": [
    "comparing_results = pd.concat([RAG_baseline_results_df, RAG_contextualized_results_df, RAG_hierarchical_results_df],axis=0)\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['method']], axis=1)\n",
    "\n",
    "mean_result_by_method = numeric_comparing_results.groupby(['method'])\\\n",
    "                                                .agg('mean').reset_index()\n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_filename = f\"results/comparing_methods_meanresults.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mean_result_by_method.to_csv(results_filename, index=False)  # `index=False` prevents writing row numbers\n",
    "\n",
    "print(\"Documents have been saved to {results_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents have been saved to {results_filename}\n"
     ]
    }
   ],
   "source": [
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['user_input']], axis=1)\n",
    "\n",
    "mean_result_by_method_userinput = numeric_comparing_results.groupby(['user_input'])\\\n",
    "                                                           .agg('mean').reset_index()   \n",
    "\n",
    "# Save the results to a JSON file\n",
    "results_filename = f\"results/comparing_methods_meanresults_byinputs.csv\"\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "mean_result_by_method_userinput.to_csv(results_filename, index=False)  # `index=False` prevents writing row numbers\n",
    "\n",
    "print(\"Documents have been saved to {results_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-chat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
