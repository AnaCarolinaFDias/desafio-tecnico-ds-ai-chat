{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import langchain\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.evaluation.qa import QAGenerateChain\n",
    "from langchain.evaluation.qa import QAEvalChain\n",
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from uuid import uuid4\n",
    "\n",
    "import openai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import getpass\n",
    "import logging\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "# Configurar o logging para salvar a saída de debug em um arquivo\n",
    "logging.basicConfig(\n",
    "    filename='./logs/debug_output.log',  # O arquivo onde os logs serão salvos\n",
    "    level=logging.DEBUG,          # O nível de log (DEBUG para capturar tudo)\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'  # Formato do log\n",
    ")\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv() # read local .env file\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "   os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'inputs/OutdoorClothingCatalog_1000.csv'\n",
    "loader = CSVLoader(file_path=file, encoding=\"utf-8\")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = TokenTextSplitter(chunk_size=200, chunk_overlap=10)\n",
    "\n",
    "split_documents = []\n",
    "for doc in docs:\n",
    "    split_docs = text_splitter.split_documents([doc])\n",
    "    split_documents.extend(split_docs)\n",
    "    \n",
    "documents_dict = [\n",
    "    {\"page_content\": doc.page_content, \"metadata\": doc.metadata} for doc in split_documents\n",
    "]\n",
    "# Save the list of dictionaries to a JSON file\n",
    "with open(\"inputs/documents_split_langchain.json\", \"w\") as file:\n",
    "    json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "logger.debug(\"Documents have been saved to 'documents_split_langchain.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents split\n",
    "with open(\"inputs/documents_split_langchain.json\", \"r\") as file:\n",
    "    documents_dict = json.load(file)\n",
    "\n",
    "# Convert the list of dictionaries back to a list of Document objects\n",
    "documents = [\n",
    "    Document(page_content=doc[\"page_content\"], metadata=doc[\"metadata\"])\n",
    "    for doc in documents_dict\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import openai_embedding_function, hf_embeddings_function, google_embedding_function \n",
    "# Generating embeddings \n",
    "openai_embedding_function(documents, model = \"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions import load_VectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI db loaded\n"
     ]
    }
   ],
   "source": [
    "embeddings_provider = 'OpenAI'\n",
    "vector_store = load_VectorStore(embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando as querys para validar os metodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5):\n",
    "    \n",
    "    logger.debug(\"Creating {select_n_documents} validation examples with {llm_model} {embeddings_provider}\")\n",
    "\n",
    "    # Initialize the example generation chain\n",
    "    example_gen_chain = QAGenerateChain.from_llm(ChatOpenAI(model=llm_model))\n",
    "\n",
    "    # Select 5 random documents from the list `docs`\n",
    "    random_documents = random.sample(docs, select_n_documents)\n",
    "\n",
    "    # Format documents for input\n",
    "    formatted_documents = [{\"doc\": doc} for doc in random_documents]\n",
    "\n",
    "    # Apply the example generation chain and parse the results\n",
    "    gen_examples = example_gen_chain.apply_and_parse(formatted_documents)\n",
    "\n",
    "    # Adjust the output to include context\n",
    "    gen_examples_adjusted_examples = []\n",
    "    for doc, example in zip(random_documents, gen_examples):\n",
    "        qa_pair = example.get('qa_pairs', {})\n",
    "        query = qa_pair.get('query', '')\n",
    "        answer = qa_pair.get('answer', '')\n",
    "\n",
    "        gen_examples_adjusted_examples.append({\n",
    "            'context': doc,  # Add the original document as context\n",
    "            'query': query,\n",
    "            'ground_truths': answer\n",
    "        })\n",
    "\n",
    "    validation_examples = gen_examples_adjusted_examples\n",
    "\n",
    "    # Prepare the results in the expected format\n",
    "    documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"ground_truths\": doc['ground_truths'], \n",
    "          \"query\": doc['query']}   \n",
    "         for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(documents_dict, file, indent=4)\n",
    "\n",
    "    logger.debug(\"Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return validation_examples\n",
    "\n",
    "validation_examples = generate_and_save_validationexamples(docs, embeddings_provider, llm_model=\"gpt-4o-mini\", select_n_documents=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparando métodos de RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Documents have been saved to 'results/query_OpenAI_baselinemethod_documents_dict_results.json'\n"
     ]
    }
   ],
   "source": [
    "def baseline_similarity_method(validation_examples, vector_store, embeddings_provider):\n",
    "\n",
    "    logger.debug(\"baseline_similarity_method to answer queries iniciated\")\n",
    "\n",
    "    model = 'gpt-3.5-turbo-instruct'\n",
    "\n",
    "    # Initialize the language model\n",
    "    llm = OpenAI(temperature=0.2, model=model)\n",
    "\n",
    "    # Set up the retriever with similarity search\n",
    "    retriever = vector_store.as_retriever(search_type=\"similarity\", k=1)  # Retrieve the most relevant chunk\n",
    "\n",
    "    # Initialize the RetrievalQA chain\n",
    "    qa = RetrievalQA.from_chain_type(\n",
    "        llm=llm, \n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever, \n",
    "        verbose=True,\n",
    "        chain_type_kwargs={\"document_separator\": \"\\n\"}\n",
    "    )\n",
    "\n",
    "    # Apply the model to the validation examples\n",
    "    predictions = qa.apply(validation_examples)\n",
    "\n",
    "    # Prepare the results\n",
    "    baselinemethod_documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "          \"metadata\": doc['context'].metadata,\n",
    "          \"query\": doc['query'],\n",
    "          \"ground_truths\": doc['ground_truths'],  \n",
    "          \"answer\": doc['result']}\n",
    "         for doc in data]\n",
    "        for data in [predictions]\n",
    "    ]\n",
    "\n",
    "    # Ensure the results directory exists\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the results to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_baselinemethod_documents_dict_results.json\"\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(baselinemethod_documents_dict, file, indent=4)\n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(\"baseline_similarity_method Documents have been saved to {results_filename}\")\n",
    "\n",
    "\n",
    "    return baselinemethod_documents_dict\n",
    "\n",
    "baselinemethod_documents_dict = baseline_similarity_method(validation_examples, vector_store, embeddings_provider)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Técnica por similaridade apenas para os documentos mais relevantes \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Some relevant categories for the Women's Chunky Neck Blanket could include:\n",
      "\n",
      "- Blankets\n",
      "- Winter Accessories\n",
      "- Women's Clothing\n",
      "- Knitwear\n",
      "- Wool Products\n",
      "\n",
      "1. Jackets\n",
      "2. Down Jackets\n",
      "3. Outdoor Clothing\n",
      "4. Winter Clothing\n",
      "5. Insulated Jackets\n",
      "6. Cold Weather Gear\n",
      "7. Performance Clothing\n",
      "8. Temperature Resistant Clothing\n",
      "9. Outdoor Gear\n",
      "10. Winter Sports Equipment\n",
      "\n",
      "1. Hunting apparel\n",
      "2. Duck hunting gear\n",
      "3. Outdoor clothing\n",
      "4. Waterproof jackets\n",
      "5. Camouflage clothing\n",
      "6. Hunting accessories\n",
      "7. Waterfowl hunting gear\n",
      "8. Duck calls\n",
      "9. Hunting jackets\n",
      "10. Hunting pants\n",
      "\n",
      "1. Shirts\n",
      "2. Cotton Shirts\n",
      "3. Chambray Shirts\n",
      "4. Easy-Care Shirts\n",
      "5. Modern Shirts\n",
      "6. Casual Shirts\n",
      "7. Button-down Shirts\n",
      "8. Short-sleeve Shirts\n",
      "9. Lightweight Shirts\n",
      "10. Comfortable Shirts\n",
      "\n",
      "1. Jackets\n",
      "2. Insulated Clothing\n",
      "3. Outerwear\n",
      "4. Pullovers\n",
      "5. Winter Clothing\n",
      "6. Cold Weather Gear\n",
      "7. Windproof Clothing\n",
      "8. Active Wear\n",
      "9. Outdoor Gear\n",
      "10. Performance Clothing\n",
      "Documents have been saved to 'results/query_OpenAI_hierarchicalmethod_documents_dict_results.json'\n"
     ]
    }
   ],
   "source": [
    "def hierarchical_retrieval_for_multiple_queries(vector_store, validation_examples):\n",
    "\n",
    "    logger.debug(\"hierarchical_retrievalmethod to answer queries iniciated\")\n",
    "\n",
    "    # Define the high-level query template for category search\n",
    "    category_prompt = \"\"\"\n",
    "    You are an expert in outdoor clothing products. Based on the following query, \n",
    "    return a list of relevant categories (e.g., jackets, pants) from the catalog.\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    category_template = PromptTemplate(input_variables=[\"query\"], template=category_prompt)\n",
    "    category_chain = LLMChain(prompt=category_template, llm=OpenAI())\n",
    "\n",
    "    # Define the refined search for documents within each category\n",
    "    document_prompt = \"\"\"\n",
    "    You are an assistant for question-answering tasks about the products from a store that sells outdoor clothing. \n",
    "    Your function is to use the catalog of retrieved context to answer the client's questions based on the following query and the category {category}.\n",
    "    Query: {query}\n",
    "    Category: {category}\n",
    "    Catalog: {catalog}\n",
    "    \"\"\"\n",
    "\n",
    "    document_template = PromptTemplate(input_variables=[\"query\", \"category\",'catalog'], template=document_prompt)\n",
    "    document_chain = LLMChain(prompt=document_template, llm=OpenAI())\n",
    "\n",
    "    for index, item in enumerate(validation_examples):\n",
    "        query = item['query']\n",
    "        category_result = category_chain.run({\"query\": query})\n",
    "\n",
    "        modified_query = f\"{query} Category: {category_result}\"  # Combine query and category for search\n",
    "        catalog = vector_store.similarity_search(modified_query, k=5) \n",
    "\n",
    "        print(category_result)\n",
    "        refined_answer = document_chain.run({\"query\": query, \"category\": category_result, \"catalog\": catalog})\n",
    "        validation_examples[index]['result'] = refined_answer\n",
    "\n",
    "    hierarchicalmethod_documents_dict = [\n",
    "        [{\"context\": doc['context'].page_content, \n",
    "        \"metadata\": doc['context'].metadata,\n",
    "        \"query\": doc['query'],\n",
    "        \"ground_truths\": doc['ground_truths'],  \n",
    "        \"answer\": doc['result'],\n",
    "        } \n",
    "        for doc in data]\n",
    "        for data in [validation_examples]\n",
    "    ]\n",
    "\n",
    "    os.makedirs('results', exist_ok=True)\n",
    "\n",
    "    # Save the list of dictionaries to a JSON file\n",
    "    results_filename = f\"results/query_{embeddings_provider}_hierarchicalmethod_documents_dict_results.json\"\n",
    "\n",
    "    with open(results_filename, \"w\") as file:\n",
    "        json.dump(hierarchicalmethod_documents_dict, file, indent=4)\n",
    "        \n",
    "\n",
    "    print(f\"Documents have been saved to '{results_filename}'\")\n",
    "    logger.debug(\"Hierarchical method Documents have been saved to {results_filename}\")\n",
    "\n",
    "    return hierarchicalmethod_documents_dict\n",
    "\n",
    "\n",
    "# Assuming vector_store is defined as per the previous setup\n",
    "hierarchicalmethod_documents_dict = hierarchical_retrieval_for_multiple_queries(vector_store, validation_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Retrieval Strategy (Hierarchical Retrieval)\n",
    "Hierarchical retrieval means you will first retrieve high-level categories or subtopics and then refine the search to get more specific documents.\n",
    "\n",
    "Step 1: Retrieve relevant high-level categories (e.g., product types like \"jackets\", \"pants\", etc.)\n",
    "Step 2: Within each category, retrieve specific documents (e.g., details of specific jackets or pants).\n",
    "To achieve this, you may use a two-step retrieval process. First, search for broad categories in the vector store, then use the results to refine the search.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método Hierarchical Retrieval: Busca Estruturada\n",
    "def hierarchical_retrieval(query, embedding, top_n=3):\n",
    "    # Categorias hipotéticas\n",
    "    categories = {\n",
    "        \"Footwear\": [\"Women's Campside Oxfords\", \"Running Shoes\"],\n",
    "        \"Dog Mats\": [\"Recycled Waterhog Dog Mat\", \"Outdoor Dog Mat\"],\n",
    "        \"Swimsuits\": [\"Infant and Toddler Girls' Coastal Chill Swimsuit\"]\n",
    "    }\n",
    "    \n",
    "    # Etapa 1: Identificar a categoria relevante com base na consulta\n",
    "    category_found = None\n",
    "    if \"dog mat\" in query.lower():\n",
    "        category_found = \"Dog Mats\"\n",
    "    elif \"shoe\" in query.lower():\n",
    "        category_found = \"Footwear\"\n",
    "    elif \"swimsuit\" in query.lower():\n",
    "        category_found = \"Swimsuits\"\n",
    "    \n",
    "    if not category_found:\n",
    "        return \"Category not found\"\n",
    "\n",
    "    # Etapa 2: Buscar documentos na categoria encontrada\n",
    "    category_docs = [doc for doc in documents_dict if category_found in doc[\"page_content\"]]\n",
    "    \n",
    "    # Etapa 3: Gerar embeddings para os documentos da categoria\n",
    "    query_embedding = embedding.embed(query)\n",
    "    category_embeddings = [embedding.embed(doc[\"page_content\"]) for doc in category_docs]\n",
    "    \n",
    "    # Calcular similaridade entre o query e os documentos da categoria\n",
    "    similarities = [\n",
    "        (i, embedding.similarity(query_embedding, doc_emb)) for i, doc_emb in enumerate(category_embeddings)\n",
    "    ]\n",
    "    \n",
    "    # Ordenar por similaridade e pegar os top N\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    top_results = [category_docs[i] for i, _ in similarities[:top_n]]\n",
    "    \n",
    "    return top_results\n",
    "\n",
    "# Exemplo de consulta\n",
    "query = \"Tell me about the dog mat\"\n",
    "results_hierarchical = hierarchical_retrieval(query)\n",
    "\n",
    "for result in results_hierarchical:\n",
    "    print(f\"Document: {result['page_content']}\")\n",
    "\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def validate_results(file_path):\n",
    "    # Read the JSON file\n",
    "    with open(file_path, \"r\") as file:\n",
    "        content = file.read()  # Read the entire content as a string\n",
    "        baseline_results = json.loads(content)\n",
    "\n",
    "    # Flatten the list of results and rename to database_result\n",
    "    database_result = [item for sublist in baseline_results for item in sublist]\n",
    "\n",
    "    # Initialize lists to store the values\n",
    "    queries = []  # List for queries\n",
    "    answer = []  # List for predicted answers\n",
    "    context = []  # List for context\n",
    "    ground_truths = []  # List for ground truth answers\n",
    "\n",
    "    # Populate the lists\n",
    "    for i, eg in enumerate(database_result):\n",
    "        queries.append(database_result[i]['query'])\n",
    "        answer.append(database_result[i]['answer'])\n",
    "        context.append([database_result[i]['context']])\n",
    "        ground_truths.append(database_result[i]['answer'])\n",
    "\n",
    "    # Create the final dictionary\n",
    "    final_dict = {\n",
    "        \"question\": queries,\n",
    "        \"answer\": answer,\n",
    "        \"contexts\": context,\n",
    "        \"ground_truth\": ground_truths\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a Dataset\n",
    "    Dataset_results = Dataset.from_dict(final_dict)\n",
    "\n",
    "    return Dataset_results\n",
    "\n",
    "# Example usage:\n",
    "file_path_baseline = \"results/query_OpenAI_baselinemethod_documents_dict_results.json\"\n",
    "RAG_baseline_results = validate_results(file_path_baseline)\n",
    "\n",
    "\n",
    "file_path_hierarchical = \"results/query_OpenAI_hierarchicalmethod_documents_dict_results.json\"\n",
    "RAG_hierarchical_results = validate_results(file_path_hierarchical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac724a7523b345ff9fcad322b65ad163",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afa13cc1fc744f388d2e1a89c68a2d50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ragas import evaluate\n",
    "from ragas.metrics import faithfulness, answer_relevancy, answer_similarity, answer_correctness, context_precision, context_recall, context_entity_recall\n",
    "\n",
    "RAG_baseline_results_metrics = evaluate(RAG_baseline_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_baseline_results_df = RAG_baseline_results_metrics.to_pandas()\n",
    "RAG_baseline_results_df['method']= 'baseline'\n",
    "\n",
    "RAG_hierarchical_results_metrics = evaluate(RAG_hierarchical_results, metrics=[faithfulness, answer_correctness, answer_relevancy,\n",
    "                                       context_precision, context_recall, context_entity_recall, \n",
    "                                       answer_similarity, answer_correctness])\n",
    "\n",
    "RAG_hierarchical_results_df = RAG_hierarchical_results_metrics.to_pandas()\n",
    "RAG_hierarchical_results_df['method']= 'hierarchical'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key features and benefits of the ...</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.970188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.261111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the key features and specifications o...</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the key features of the Burnt Oak Qua...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the key features of the Easy-Care Cot...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key features of the Women's Chunk...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  faithfulness  \\\n",
       "0  What are the key features and benefits of the ...      0.725000   \n",
       "1  What are the key features and specifications o...      0.958333   \n",
       "2  What are the key features of the Burnt Oak Qua...      0.857143   \n",
       "3  What are the key features of the Easy-Care Cot...      0.947368   \n",
       "4  What are the key features of the Women's Chunk...      1.000000   \n",
       "\n",
       "   answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
       "0            0.965909          0.970188                1.0        0.833333   \n",
       "1            1.000000          0.958467                1.0        1.000000   \n",
       "2            1.000000          0.969803                1.0        0.900000   \n",
       "3            1.000000          0.975345                1.0        0.750000   \n",
       "4            1.000000          0.953991                1.0        1.000000   \n",
       "\n",
       "   context_entity_recall  semantic_similarity  \n",
       "0               0.261111             1.000000  \n",
       "1               0.583333             0.999998  \n",
       "2               0.773810             1.000000  \n",
       "3               0.312500             1.000000  \n",
       "4               0.562500             1.000000  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparing_results = pd.concat([RAG_baseline_results_df, RAG_hierarchical_results_df],axis=0)\n",
    "\n",
    "# Select only numeric columns\n",
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['method']], axis=1)\n",
    "\n",
    "numeric_comparing_results.groupby(['method'])\\\n",
    "                 .agg('mean').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>faithfulness</th>\n",
       "      <th>answer_correctness</th>\n",
       "      <th>answer_relevancy</th>\n",
       "      <th>context_precision</th>\n",
       "      <th>context_recall</th>\n",
       "      <th>context_entity_recall</th>\n",
       "      <th>semantic_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the key features and benefits of the ...</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.965909</td>\n",
       "      <td>0.970188</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.261111</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the key features and specifications o...</td>\n",
       "      <td>0.958333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.958467</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.999998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the key features of the Burnt Oak Qua...</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.969803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.773810</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the key features of the Easy-Care Cot...</td>\n",
       "      <td>0.947368</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.975345</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What are the key features of the Women's Chunk...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.953991</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  faithfulness  \\\n",
       "0  What are the key features and benefits of the ...      0.725000   \n",
       "1  What are the key features and specifications o...      0.958333   \n",
       "2  What are the key features of the Burnt Oak Qua...      0.857143   \n",
       "3  What are the key features of the Easy-Care Cot...      0.947368   \n",
       "4  What are the key features of the Women's Chunk...      1.000000   \n",
       "\n",
       "   answer_correctness  answer_relevancy  context_precision  context_recall  \\\n",
       "0            0.965909          0.970188                1.0        0.833333   \n",
       "1            1.000000          0.958467                1.0        1.000000   \n",
       "2            1.000000          0.969803                1.0        0.900000   \n",
       "3            1.000000          0.975345                1.0        0.750000   \n",
       "4            1.000000          0.953991                1.0        1.000000   \n",
       "\n",
       "   context_entity_recall  semantic_similarity  \n",
       "0               0.261111             1.000000  \n",
       "1               0.583333             0.999998  \n",
       "2               0.773810             1.000000  \n",
       "3               0.312500             1.000000  \n",
       "4               0.562500             1.000000  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_comparing_results = pd.concat([comparing_results.select_dtypes(include='number'), comparing_results['user_input']], axis=1)\n",
    "\n",
    "numeric_comparing_results.groupby(['user_input'])\\\n",
    "                 .agg('mean').reset_index()   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-ai-chat-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
